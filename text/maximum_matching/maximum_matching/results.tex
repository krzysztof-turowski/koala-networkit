\section{Code structure}

All described algorithms have been implemented as part of the KOALA library~\cite{koala-networkit} built on top of NetworKit~\cite{networkit}. 

The implementations are contained in following files:

\begin{itemize}
    \item \texttt{include/matching/MaximumMatching.hpp} contains declarations of classes corresponding to maximum matching algorithms, namely: \begin{itemize}
        \item \texttt{MaximumWeightMatching} – the base class for \textsc{MWM} and \textsc{MWPM} algorithms,
        \item \texttt{BlossomMaximumMatching} – an abstract base class for implementations of the blossom algorithm. Responsible for the common operations shared by all implementations – building the search tree, creating blossoms, augmenting paths, expanding blossom and others. It relies on call virtual methods implemented by children classes to perform implementation-specific parts of the algorithm. One such part is calculating the $\delta_i$ values during dual weight adjustment. Another one is finding useful edges to perform the next step. After each step of the algorithm, specific virtual methods are called to allow the implementations to update their specific data structures.
        \item \texttt{EdmondsMaximumMatching} – the Edmonds' original $O(n^2m)$ version of the blossom algorithm~\cite{edmonds1965maximum},
        \item \texttt{GabowMaximumMatching} – the Gabow's $O(n^3)$ implementation of the blossom algorithm~\cite{gabow1974implementation},
        \item \texttt{GalilMicaliGabowMaximumMatching} – the $O(nm\log n)$ implementation of the blossom algorithm of~\cite{galil1986ev},
        \item \texttt{GabowScalingMatching} – the $O(n^{3/4}m\log N)$ scaling algorithm~\cite{gabow1984scaling},
        \item \texttt{MaximumCardinalityMatching} – the base class for \textsc{MCM} algorithms,
        \item \texttt{MicaliVaziraniMatching} – the class for the implementation of the Micali-Vazirani maximum cardinality matching algorithm~\cite{micali1980v}.
    \end{itemize}
    \item \texttt{include/matching/structures} directory contains the implementations of various data structures utilized across the implemented algorithms: \begin{itemize}
        \item \texttt{ArrayPriorityQueue.hpp} – contains the implementation of a simple array priority queue used by the scaling algorithm,
        \item \texttt{ConcatenableQueue.hpp} – contains the implementation of concatenable queues,
        \item \texttt{FenwickTree.hpp} – contains an implementation of the $\textsc{add-prefixsum}$ data strucutre based the Fenwick trees,
        \item \texttt{HeapWithRemove.hpp} – contains a simple implementation of an array based heap with ability to remove elements, used by the $pq_1$ priority queue implementation,
        \item \texttt{PriorityQueues.hpp} – contains the implementations \texttt{PriorityQueue1} and \texttt{PriorityQueue2} of the $pq_1$ and $pq_2$ priority queues,
        \item \texttt{SplitFindMin.hpp} – contains the splitting list implementation of the $\textsc{split-findmin}$ data structure,
        \item \texttt{UnionFind.hpp} – contains the implementation of the $\textsc{union-find}$ data structure.
    \end{itemize}
    \item \texttt{cpp/matching} contains the implementation of the classes defined in the \texttt{MaximumMatching.hpp} header file,
    \item \texttt{test/testMaximumMatching.cpp} contains simple unit tests for the implemented algorithms,
    \item \texttt{benchmark/benchmarkMaximumMatching.cpp} contains the program used to benchmark the various algorithms on provided graphs with various options.
\end{itemize}

All tests were conducted on a personal computer equipped with an Intel Core i7-6700HQ processor running at a clock speed of 2.60 GHz, using the Ubuntu 22.04.4 LTS operating system.

We've made use of test generators provided as part of the 1st DIMACS implementation challenge~\cite{rutgersDIMACSImplementation}.

\section{Tests}

\subsection{Random instances}

The first class of graphs we've tested are random sparse and dense graphs generated using the \texttt{random.c} generator from the DIMACS challenge written by C. McGeoch. The program generates a random graph with a provided number of nodes and edges with weights uniformly chosen from a specified range $[1, N]$. It is also possible to specify a seed used to generate random numbers. For all of our test we've set the maximum cost at $2^{16}$. For each pair of values $n$ and $m$ we've generated $3$ different graphs using random seeds and then average the running time.

Our set of sparse graphs consists of graphs with varying values of $n$ and $m = a n$ where $a \in \{2, 4, 8 \}$. The results are presented in~\Cref{tab:sparse}. We use the letter $E$ to refer to our implementation of Edmonds' algorithm, $G$ to Gabow's blossom algorithm, $M$ to Galil, Micali and Gabow's algorithm and $S$ to Gabow's scaling algorithm. We will refer to them using those letters. 
As our set of dense graphs we've chosen random graphs with $n \in \{ 1000, 2000, 3000, 4000 \}$ and containing approximately $20\%$, $40\%$ and $60\%$ possible edges with results presented in~\Cref{tab:dense}. In both cases the scaling algorithm performs the best. Among our implementations of the blossom algorithm, $M$ runs fastest for some smaller test, but gets overtaken by $E$ for larger instances. The $G$ implementation is consistently the slowest for sparse graphs and on par with $M$ for dense instances.

We plot the running time of tested algorithms in relation to $n$ for sparse graphs with $m = 4 n$ and dense graphs containing roughly $20\%$ of possible edges. For sparse graphs, the running time of blossom algorithm implementation is roughly quadratic, indicating that that the $G$ and $E$ variant perform better than their theoretical worst case complexity would suggest. Meanwhile, the running time of the scaling algorithm $S$ seems to align with its $n^{7/4}$ complexity. For dense instances, gauging the asymptotic running time seems to be harder with the blossom implementations taking roughly $n^3$ time. 


\begin{table}
\centering
\begin{tabular}{
cc|cccc}
$n$ & $m$ & $E$ & $G$ & $M$ & $S$ \\
\hline
1000 & 2000 & 0.15 & 0.17 & \textbf{0.11} & 0.20 \\
1000 & 4000 & 0.28 & 0.25 & \textbf{0.19} & 0.25 \\
1000 & 8000 & 0.57 & 0.39 & 0.33 & \textbf{0.29} \\
2000 & 4000 & 0.59 & 0.76 & \textbf{0.43} & 0.44 \\
2000 & 8000 & 1.17 & 1.22 & 0.77 & \textbf{0.59} \\
2000 & 16000 & 2.14 & 1.75 & 1.37 & \textbf{0.69} \\
4000 & 8000 & 2.39 & 3.33 & 1.86 & \textbf{1.29} \\
4000 & 16000 & 4.54 & 5.35 & 3.47 & \textbf{1.72} \\
4000 & 32000 & 8.61 & 8.15 & 6.56 & \textbf{2.73} \\
8000 & 16000 & 9.48 & 14.91 & 11.64 & \textbf{4.05} \\
8000 & 32000 & 18.30 & 27.09 & 22.72 & \textbf{5.41} \\
8000 & 64000 & 34.24 & 50.38 & 41.95 & \textbf{8.77} \\
16000 & 32000 & 43.03 & 93.19 & 73.50 & \textbf{11.72} \\
16000 & 64000 & 88.41 & 171.60 & 145.75 & \textbf{16.00} \\
16000 & 128000 & 149.67 & 294.83 & 264.03 & \textbf{33.65} \\
32000 & 64000 & 232.85 & 569.88 & 410.78 & \textbf{44.63} \\
32000 & 128000 & 414.49 & 1026.40 & 790.05 & \textbf{52.79} \\
32000 & 256000 & 611.80 & 1652.81 & 1355.59 & \textbf{125.09} \\
\end{tabular}
\caption{Results on random sparse graphs.}\label{tab:sparse}
\end{table}

\begin{table}
\centering
\begin{tabular}{
cc|cccc}
$n$ & $m$ & $E$ & $G$ & $M$ & $S$ \\
\hline
1000 & 100000 & 5.65 & 3.51 & 4.08 & \textbf{0.82} \\
1000 & 200000 & 7.35 & 7.26 & 6.87 & \textbf{1.62} \\
1000 & 300000 & 8.04 & 10.26 & 10.32 & \textbf{2.26} \\
2000 & 400000 & 25.09 & 40.97 & 43.96 & \textbf{4.47} \\
2000 & 800000 & 31.16 & 60.80 & 55.41 & \textbf{8.14} \\
2000 & 1200000 & 40.67 & 87.33 & 87.49 & \textbf{12.77} \\
3000 & 900000 & 82.91 & 149.13 & 160.21 & \textbf{12.36} \\
3000 & 1800000 & 84.38 & 205.11 & 214.92 & \textbf{17.59} \\
3000 & 2700000 & 102.76 & 286.73 & 278.40 & \textbf{50.79} \\
4000 & 1600000 & 139.25 & 346.93 & 383.64 & \textbf{23.99} \\
4000 & 3200000 & 151.17 & 505.38 & 503.95 & \textbf{38.57} \\
4000 & 4800000 & 233.07 & 700.78 & 775.19 & \textbf{99.25} \\
\end{tabular}
\caption{Results on random dense graphs.}\label{tab:dense}
\end{table}

\begin{figure}[b]
    \centering
    \includegraphics*[width=0.8\textwidth]{figures/sparse.png}
    \includegraphics*[width=0.8\textwidth]{figures/dense.png}
    \caption{Running time of tested algorithms on sparse and dense random instances.}\label{fig:sparse_dense}
\end{figure}

\subsubsection*{Perfect matching}

We repeat the same tests, this time running the perfect matching versions of our implementations. The tests are generated similarly as in the previous section, with the difference that we make sure that the generated instances contain perfect matchings by running our implementation of the Micali-Vazirani maximum cardinality algorithm. The approach was infeasible for some larger sparse instances, in which case we've used our own random test generator that ensures that the generated graph contains a perfect matching by first generating a random permutation $v_1, v_2, \dots v_n$ of vertices $[1, n]$  and adding edges $v_1v_2, v_3v_4, \dots, v_{n-1}v_n$, with the rest of edges chosen randomly.

We've tested our blossom algorithm implementation with both the empty and greedy initialization strategies. The results using the greedy initialization are marked with a $G$ subscript. Additionally, all algorithms are marked with $^*$ to indicate that the perfect matching variant is used.

The results of our experiments on random sparse and dense graphs containing perfect matchings are presented in~\Cref{tab:perfect}. As in the case of non-perfect matchings, the scaling algorithm $S$ comes out on top. The margin is even larger most likely to the fact it is designed to find maximum perfect matching and finding a non-perfect matching requires a reduction that doubles the number of nodes and edges. For all blossom algorithm implementations, the greedy initialization resulted in a shorter running time. The speedup ranged from around $30\%$ for the $E$ algorithm and as much as 2-4 times for the $G$ and $M$ implementations.

\begin{table}
\centering
\rotatebox{90} {
\begin{tabular}{
cc|ccccccc}
$n$ & $m$ & $E^*$ & $E^*_G$ & $G^*$ & $G^*_G$ & $M^*$ & $M^*_G$ & $S^*$ \\
\hline
1000 & 4000 & 0.32 & 0.22 & 0.27 & 0.13 & 0.19 & \textbf{0.08} & 0.13 \\
1000 & 8000 & 0.54 & 0.36 & 0.36 & 0.16 & 0.33 & \textbf{0.13} & \textbf{0.13} \\
2000 & 8000 & 1.36 & 0.90 & 1.25 & 0.65 & 0.80 & 0.33 & \textbf{0.29} \\
2000 & 16000 & 2.11 & 1.43 & 1.63 & 0.74 & 1.35 & 0.51 & \textbf{0.30} \\
4000 & 16000 & 5.52 & 3.91 & 5.66 & 3.07 & 3.55 & 1.38 & \textbf{0.73} \\
4000 & 32000 & 9.15 & 5.91 & 7.75 & 3.42 & 6.61 & 2.45 & \textbf{0.93} \\
8000 & 32000 & 21.34 & 15.33 & 29.52 & 13.81 & 22.47 & 8.85 & \textbf{2.24} \\
8000 & 64000 & 35.71 & 23.37 & 48.09 & 16.29 & 42.31 & 15.19 & \textbf{3.31} \\
16000 & 64000 & 100.73 & 74.28 & 184.69 & 87.47 & 147.05 & 54.72 & \textbf{7.66} \\
16000 & 128000 & 156.35 & 106.55 & 298.43 & 107.97 & 266.19 & 93.96 & \textbf{11.38} \\
32000 & 128000 & 512.00 & 354.36 & 1166.83 & 594.43 & 767.65 & 283.84 & \textbf{26.70} \\
32000 & 256000 & 707.58 & 542.16 & 1655.12 & 727.57 & 1381.63 & 528.16 & \textbf{21.80} \\
\midrule
$n$ & $m$ & $E^*$ & $E^*_G$ & $G^*$ & $G^*_G$ & $M^*$ & $M^*_G$ & $S^*$ \\
\hline
1000 & 100000 & 4.79 & 3.75 & 3.54 & 0.74 & 4.21 & 1.27 & \textbf{0.41} \\
1000 & 200000 & 6.49 & 4.47 & 7.11 & 1.67 & 6.60 & 2.11 & \textbf{0.70} \\
1000 & 300000 & 8.14 & 5.68 & 10.98 & 2.72 & 10.29 & 3.32 & \textbf{0.89} \\
2000 & 400000 & 28.21 & 22.42 & 41.09 & 10.21 & 44.07 & 13.51 & \textbf{1.93} \\
2000 & 800000 & 33.37 & 26.92 & 62.93 & 16.37 & 56.31 & 18.27 & \textbf{2.83} \\
2000 & 1200000 & 40.55 & 30.74 & 86.72 & 22.38 & 84.48 & 26.57 & \textbf{4.20} \\
4000 & 1600000 & 134.92 & 110.14 & 347.76 & 87.46 & 389.26 & 114.10 & \textbf{10.89} \\
4000 & 3200000 & 157.18 & 111.22 & 494.36 & 127.89 & 504.29 & 147.19 & \textbf{16.26} \\
4000 & 4800000 & 218.07 & 143.22 & 718.58 & 180.23 & 786.43 & 227.36 & \textbf{32.61} \\
\end{tabular}
}
\caption{Results on random sparse and dense graphs when looking for a perfect matching.}\label{tab:perfect}
\end{table}

\subsection{Influence of edge weights}

The previous tests have neglected to account for one of the main contributing factors in the running time of the scaling algorithm - the maximum edge weight $N$. To test how different values of $N$ affect the running time of the $S$ algorithm, we've conducted a series of tests on random sparse graphs with $m = 4n$ and values of $N$ ranging from $1$ to $10^7$. We've compared both the perfect and non-perfect variant of $S$ to the $E$ algorithm. 

The results can be found in~\Cref{tab:weight}. We can observe that the running time of the blossom algorithm implementation $E$ is influenced slightly by the value of $N$ but seems to level off at higher values of $N$. Intuitively, we speculate that the initial increase in weight range causes a rise in complexity, but as the weights get larger, especially compared to the number of edges, the higher weight range doesn't meaningfully increase the complexity. As we expect from its time complexity, the running time of the $S$ algorithm seems to scale with $\log N$, which can be seen in~\Cref{fig:weight}.

\begin{table}
\centering
\begin{tabular}{
ccc|cc|cc}
$n$ & $m$ & $N$ & $E$ & $S$ & $E^*_G$ & $S^*$ \\
\hline
1000 & 4000 & 1 & 0.07 & \textbf{0.01} & \textbf{0.01} & \textbf{0.01} \\
1000 & 4000 & 10 & \textbf{0.06} & 0.07 & \textbf{0.03} & 0.04 \\
1000 & 4000 & 100 & \textbf{0.08} & 0.11 & 0.07 & \textbf{0.06} \\
1000 & 4000 & 1000 & 0.19 & \textbf{0.16} & 0.17 & \textbf{0.09} \\
1000 & 4000 & 10000 & 0.28 & \textbf{0.21} & 0.23 & \textbf{0.11} \\
1000 & 4000 & 100000 & 0.29 & \textbf{0.24} & 0.24 & \textbf{0.12} \\
1000 & 4000 & 1000000 & 0.30 & \textbf{0.26} & 0.29 & \textbf{0.15} \\
1000 & 4000 & 10000000 & \textbf{0.29} & 0.33 & 0.22 & \textbf{0.16} \\
\hline
2000 & 8000 & 1 & 0.27 & \textbf{0.03} & \textbf{0.02} & \textbf{0.02} \\
2000 & 8000 & 10 & 0.23 & \textbf{0.15} & 0.11 & \textbf{0.09} \\
2000 & 8000 & 100 & \textbf{0.25} & 0.27 & 0.22 & \textbf{0.15} \\
2000 & 8000 & 1000 & 0.55 & \textbf{0.41} & 0.54 & \textbf{0.24} \\
2000 & 8000 & 10000 & 1.05 & \textbf{0.50} & 0.85 & \textbf{0.25} \\
2000 & 8000 & 100000 & 1.17 & \textbf{0.59} & 0.90 & \textbf{0.29} \\
2000 & 8000 & 1000000 & 1.19 & \textbf{0.68} & 1.02 & \textbf{0.33} \\
2000 & 8000 & 10000000 & 1.20 & \textbf{0.70} & 1.06 & \textbf{0.36} \\
\hline
4000 & 16000 & 1 & 1.12 & \textbf{0.07} & 0.09 & \textbf{0.04} \\
4000 & 16000 & 10 & 0.90 & \textbf{0.42} & 0.44 & \textbf{0.26} \\
4000 & 16000 & 100 & 0.91 & \textbf{0.78} & 0.68 & \textbf{0.36} \\
4000 & 16000 & 1000 & 1.64 & \textbf{1.14} & 1.65 & \textbf{0.50} \\
4000 & 16000 & 10000 & 3.75 & \textbf{1.51} & 3.17 & \textbf{0.63} \\
4000 & 16000 & 100000 & 4.72 & \textbf{1.75} & 3.96 & \textbf{0.75} \\
4000 & 16000 & 1000000 & 4.79 & \textbf{1.97} & 4.04 & \textbf{0.83} \\
4000 & 16000 & 10000000 & 4.74 & \textbf{2.33} & 3.82 & \textbf{0.87} \\
\hline
8000 & 32000 & 1 & 4.76 & \textbf{0.24} & 0.36 & \textbf{0.12} \\
8000 & 32000 & 10 & 3.68 & \textbf{1.23} & 1.90 & \textbf{0.73} \\
8000 & 32000 & 100 & 3.55 & \textbf{2.35} & 2.67 & \textbf{1.07} \\
8000 & 32000 & 1000 & 5.19 & \textbf{3.45} & 6.93 & \textbf{1.58} \\
8000 & 32000 & 10000 & 13.16 & \textbf{4.49} & 13.62 & \textbf{1.70} \\
8000 & 32000 & 100000 & 18.89 & \textbf{5.28} & 14.74 & \textbf{2.05} \\
8000 & 32000 & 1000000 & 19.66 & \textbf{6.06} & 16.81 & \textbf{2.36} \\
8000 & 32000 & 10000000 & 19.70 & \textbf{6.76} & 16.62 & \textbf{2.75} \\
\end{tabular}
\caption{Results on random graphs with varying maximum weights for both perfect and non-perfect variants.}\label{tab:weight}
\end{table}

\begin{figure}
    \centering
    \includegraphics*[width=0.8\textwidth]{figures/Nweight.png}
    \includegraphics*[width=0.8\textwidth]{figures/Eweight.png}
    \caption{Running time of the scaling algorithm and the Edmonds' algorithm in relation to the maximum edge weight $N$.}\label{fig:weight}
\end{figure}

\subsection{DIMACS instances}

We next turn to more structured instances. The tests were generated using the following programs:

\begin{itemize}
    \item \texttt{t.f} written by N. Ritchey and B. Mattingly. For a parameter $k$ it generates a sequence of $k$ triangles. Consecutive triangles are connected with a single edge. The configuration tends to generate a lot of blossoms according to the comments provided with the generators.
    \item \texttt{tt.f} written by N. Ritchey and B. Mattingly. Similar to \texttt{t.f} with the difference that each pair of consecutive triangles is connected with $3$ separate edges.
    \item \texttt{hardcard.f} written by B. Mattingly. It generates graphs that have been shown in~\cite{gabow1976efficient} to be hard for Edmonds' maximum cardinality matching algorithm.
\end{itemize}

All of these instances contain perfect matching, so we've run both variants of the algorithms. For perfect matching we've opted to only test with greedy initialization. We ran each algorithm $3$ times and average the time spent.

The results for \texttt{t.f} are presented in~\Cref{tab:t}. Once again, $S$ is by far the fastest. Of the blossom algorithm implementations $E$ is the clear winner with $G$ and $M$ following. The results are similar for \texttt{tt.f} with the exception that the greedy initialization tends to find the perfect matching immediately. 

In the case of \texttt{hardcard.f}, as observed in~\Cref{tab:hardcard}, the $S$ algorithm once again comes out on top. What is unusual is that $G$ outperforms both $E$ and $M$, while in most of the previous tests it was consistently the slowest.

\begin{table}
\centering
\begin{tabular}{
ccc|cccc}
$k$ & $n$ & $m$ & $E$ & $G$ & $M$ & $S$ \\
\hline
100 & 300 & 399 & \textbf{0.00} & 0.01 & 0.01 & \textbf{0.00} \\
200 & 600 & 799 & 0.02 & 0.02 & 0.03 & \textbf{0.00} \\
400 & 1200 & 1599 & 0.06 & 0.10 & 0.09 & \textbf{0.01} \\
1000 & 3000 & 3999 & 0.35 & 0.60 & 0.50 & \textbf{0.04} \\
2000 & 6000 & 7999 & 1.38 & 2.40 & 2.31 & \textbf{0.12} \\
4000 & 12000 & 15999 & 6.01 & 10.38 & 14.08 & \textbf{0.39} \\
\hline
$k$ & $n$ & $m$ & $E^*_G$ & $G^*_G$ & $M^*_G$ & $S^*_G$ \\
\hline
100 & 300 & 399 & \textbf{0.00} & \textbf{0.00} & \textbf{0.00} & \textbf{0.00} \\
200 & 600 & 799 & \textbf{0.00} & \textbf{0.00} & 0.01 & \textbf{0.00} \\
400 & 1200 & 1599 & \textbf{0.01} & 0.02 & 0.04 & \textbf{0.01} \\
1000 & 3000 & 3999 & 0.05 & 0.10 & 0.16 & \textbf{0.02} \\
2000 & 6000 & 7999 & 0.19 & 0.40 & 0.74 & \textbf{0.06} \\
4000 & 12000 & 15999 & 0.74 & 1.76 & 4.29 & \textbf{0.20} \\
\end{tabular}
\caption{Results on the \texttt{t.f} DIMACS instances with varying $k$.}\label{tab:t}
\end{table}

\begin{table}
\centering
\begin{tabular}{ccc|cccc}
$k$ & $n$ & $m$ & $E$ & $G$ & $M$ & $S$ \\
\hline
50 & 300 & 20000 & 0.11 & 0.08 & 0.10 & \textbf{0.02} \\
100 & 600 & 80000 & 0.90 & 0.60 & 0.75 & \textbf{0.06} \\
200 & 1200 & 320000 & 8.60 & 5.56 & 7.32 & \textbf{0.24} \\
400 & 2400 & 1280000 & 68.91 & 44.53 & 63.66 & \textbf{0.97} \\
800 & 4800 & 5120000 & 529.36 & 351.33 & 595.68 & \textbf{4.08} \\
\hline
$k$ & $n$ & $m$ & $E^*_G$ & $G^*_G$ & $M^*_G$ & $S^*$ \\
\hline
50 & 300 & 20000 & 0.03 & \textbf{0.01} & 0.04 & \textbf{0.01} \\
100 & 600 & 80000 & 0.21 & 0.07 & 0.25 & \textbf{0.03} \\
200 & 1200 & 320000 & 2.55 & 0.69 & 2.56 & \textbf{0.13} \\
400 & 2400 & 1280000 & 19.67 & 5.43 & 23.45 & \textbf{0.55} \\
800 & 4800 & 5120000 & 139.87 & 41.25 & 244.50 & \textbf{2.28} \\
\end{tabular}
\caption{Results on the \texttt{hardcard.f} DIMACS instances with varying $k$.}\label{tab:hardcard}
\end{table}

\section{Conclusions}

We have found our implementation of the scaling algorithm to be superior to our various implementations of the blossom algorithm.  We have found that better theoretical running time doesn't necessarily result in faster execution in practice. The asymptotic time of of the blossom implementation is better than the theoretical one, suggesting the worst cases rarely happen. 

Our implementations of the various iterations of the blossom algorithm are reasonably close to how they were described originally. The variants of the blossom algorithm mainly differ by how they implement distinct aspects of the search procedure: representing blossoms, maintaining dual weights, performing adjustments and finding tight edges. One method that was found to lead to good performance is the variable $\delta$ approach introduced in~\cite{cook1999computing} that perform dual adjustment by choosing a different value of $\delta$ for each search tree using a heuristic. The authors of~\cite{mehlhorn2002implementation} showed an efficient variant of the $O(nm \log n)$ algorithm that uses a combination of concatenable queues and union-find similarly to how splitting lists and union-find are used by us in the scaling algorithm. The current best performing implementation presented in~\cite{kolmogorov2009blossom} is based on the variable $\delta$ approach and uses priority queues to maintain edge slacks. The authors don't give a proof of a theoretical time complexity but believe the worst-case to be $O(n^3m)$.

We compared the performance of our scaling algorithm implementation to the implementation of~\cite{kolmogorov2009blossom} and found it to be significantly slower. While there are some ideas from existing blossom algorithm implementations that could improve the performance of our scaling algorithm, we doubt it would be enough to outperform the best implementations of the blossom algorithm. The main disadvantages of the scaling approach is that it is sensitive to sizes of weight and does a lot of repeated work by finding a new matching in each scale.
